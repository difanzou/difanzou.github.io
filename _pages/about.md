---
permalink: /
excerpt: "About me"
title: ""
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---


I am an assistant professor in the [Department of Computer Science](https://www.cs.hku.hk/) 
at [The University of Hong Kong](https://www.hku.hk/) and [The HKU Musketeers Foundation Institute of Data Science](https://datascience.hku.hk/). I am generally interested in machine learning, stochastic optimization, and graph learning, with a special focus on the theoretical/empirical understanding of the deep learning algorithms. I am also open to any challenging research topic in artifical intelligence and data science.

Previously, I obtained my Ph.D. in the Computer Science department at the University of California, Los Angeles (UCLA), supervised by [Prof. Quanquan Gu](http://web.cs.ucla.edu/~qgu/).  I obtained my master degree in electrical engineering and bachlor degree in applied physics, both in Unversity of Science and Technology of China (USTC). 


News
------

* <span style="color:red"> Multiple openings for PhD, Postdoc, interns, and RA. Please drop me an email with your CV and transcript (optional) if you are interesed in joining my research group.   </span>

* \[2024-05\] Four papers on [handling spurious correlation via better group classifier](./), [transformer expressive power with varying layers](./), [faster rate of stochastic proximal sampler](./), and [benign overfitting for XOR data](./) are accepted to ICML 2024.

* \[2024-04\] One paper on [continual learning for GNN](./) are accepted to CoLLAs 2024.



* \[2024-01\] Three papers on [SGD with large learning rate](https://arxiv.org/pdf/2310.17074.pdf), [scalable training of dynamic GNN](https://difanzou.github.io/), and [finite sample in-context learning](https://arxiv.org/pdf/2310.08391.pdf) are accepted to ICLR 2024.

* \[2024-01\] A new manuscript on [faster non-log-concave sampling](https://arxiv.org/pdf/2401.06325.pdf) is online, where we show that non-exponential convergence can be achieved for sampling non-log-concave distributions (or distribution without isoperimetry) via diffusion-based Monte Carlo. Any comments are welcomed!

* \[2023-10\] A new manuscript on [the Oscillation Effect of SGD](https://arxiv.org/pdf/2310.17074.pdf) is online, where we theoretically investigate the benefit of the oscillation of SGD with large learning rate for genralization. Any comments are welcomed! 


* \[2023-10\] A new manuscript on [Learning XOR data via ReLU networks](https://arxiv.org/pdf/2310.01975.pdf) is online, where we provide a precise analysis on the generalization performance of ReLU CNN trained by GD on (non-orthogonal) XOR-type data. Any comments are welcomed! 

* \[2023-06\] We are looking for  PostDocs in Mathematical Data Science. Candidates with a strong background in mathematics, proven publication record and interest in theoretical aspects of machine learning are encouraged to apply. Strong candidates can be referred to [HKU IDS PostDoc Research Fellowship](https://datascience.hku.hk/research/postdoctoral-fellowship/) (up to HKD 514800/year plus additional research funding support). Please find the details in the [advertisement](https://manchungyue.com/IDS_Postdoc_Ad.pdf). 

* \[2023-05\] Our [paper](https://arxiv.org/pdf/2306.11680.pdf) on the implicit bias of batch normalization is accepted to COLT 2023, manustripts will be released soon!

* \[2023-03\] Two manuscripts on explaning the advantages of [Mixup](https://arxiv.org/pdf/2303.08433.pdf) and [Gradient Regularization](https://arxiv.org/pdf/2303.17940.pdf) in training neural networks are online. Any comments are welcomed!

* \[2023-03\] I will serve as the Area Chair in NeurIPS 2023.

* \[2023-01\] Our [paper](https://openreview.net/pdf?id=iUYpN14qjTF) on the generalization separation between Adam and GD has been accepted by ICLR 2023.

* \[2022-09\] Two papers accepted by NeurIPS 2022. The [first paper](https://openreview.net/pdf?id=f966GJIEF9) studies the generalization of multi-pass SGD for over-parameterized least squres; the [second paper](https://openreview.net/pdf?id=3y80RPgHL7s) demonstrates the power and limitation of pretraining-finetunning for linear regression with distribution shift.


* \[2022-08\] Dr Difan Zou just joined HKU CS as an assistant professor.


