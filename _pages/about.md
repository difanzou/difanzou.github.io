---
permalink: /
excerpt: "About me"
title: ""
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---


I am an assistant professor in the [Department of Computer Science](https://www.cs.hku.hk/) 
at [The University of Hong Kong](https://www.hku.hk/) and [The HKU Musketeers Foundation Institute of Data Science](https://datascience.hku.hk/). I am generally interested in machine learning, stochastic optimization, and graph learning, with a special focus on the theoretical/empirical understanding (or Physics) of deep learning (especially foundation models). I am also particularlly interested in devloping the AI/ML methods for practical problems in other area, such as signal processing, intelligent transportation, and math problems.



Previously, I obtained my Ph.D. in the Computer Science department at the University of California, Los Angeles (UCLA), supervised by [Prof. Quanquan Gu](http://web.cs.ucla.edu/~qgu/).  I obtained my master degree in electrical engineering and bachelor degree in applied physics, both in Unversity of Science and Technology of China (USTC). 


News
------

* <span style="color:red"> Multiple openings for PhD, Postdoc, and RA. Please drop me an email with your CV and transcript (optional) if you are interesed in joining my research group.   </span>

* \[2024-06\] Receiving an UGC ECS funding on diffusion-based Monte Carlo.

* \[2024-06\] One paper on [faster diffusion inference/sampling](https://arxiv.org/pdf/2405.16387) is accepted to SPIGM @ ICML workshop as an oral presentation.

* \[2024-05\] One paper on [faster non-log-concave sampling](https://arxiv.org/pdf/2401.06325.pdf) is accepted to COLT 2024.

* \[2024-05\] Four papers on [handling spurious correlation via better group classifier](./), [transformer expressive power with varying layers](./), [faster rate of stochastic proximal sampler](./), and [benign overfitting for XOR data](./) are accepted to ICML 2024.

* \[2024-04\] One paper on [continual learning for GNN](./) are accepted to CoLLAs 2024.

* \[2024-04\] Receiving an Guangdong NSF funding on theoretical foundation of diffusion models.


* \[2024-01\] Three papers on [SGD with large learning rate](https://arxiv.org/pdf/2310.17074.pdf), [scalable training of dynamic GNN](https://difanzou.github.io/), and [finite sample in-context learning](https://arxiv.org/pdf/2310.08391.pdf) are accepted to ICLR 2024.

* \[2023-08\] Receiving an NSFC funding on the theoretical foundation of SGD in deep learning.

* \[2023-05\] Our [paper](https://arxiv.org/pdf/2306.11680.pdf) on the implicit bias of batch normalization is accepted to COLT 2023.

* \[2023-03\] Two manuscripts on explaning the advantages of [Mixup](https://arxiv.org/pdf/2303.08433.pdf) and [Gradient Regularization](https://arxiv.org/pdf/2303.17940.pdf) in training neural networks are online.

* \[2023-03\] I will serve as the Area Chair in NeurIPS 2023.

* \[2023-01\] Our [paper](https://openreview.net/pdf?id=iUYpN14qjTF) on the generalization separation between Adam and GD has been accepted by ICLR 2023.

* \[2022-09\] Two papers accepted by NeurIPS 2022. The [first paper](https://openreview.net/pdf?id=f966GJIEF9) studies the generalization of multi-pass SGD for over-parameterized least squres; the [second paper](https://openreview.net/pdf?id=3y80RPgHL7s) demonstrates the power and limitation of pretraining-finetunning for linear regression with distribution shift.


* \[2022-08\] Dr Difan Zou just joined HKU CS as an assistant professor.


